<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Raymond Wang&#39;s Blog</title>
<link>https://your-website-url.example.com/</link>
<atom:link href="https://your-website-url.example.com/index.xml" rel="self" type="application/rss+xml"/>
<description>A blog built with Quarto</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Wed, 14 Jan 2026 08:00:00 GMT</lastBuildDate>
<item>
  <title>Understanding the Road: Computer Vision Technology in Autonomous Vehicles</title>
  <dc:creator>Raymond Wang</dc:creator>
  <link>https://your-website-url.example.com/posts/Understanding the Road: Computer Vision Technology in Autonomous Vehicles/</link>
  <description><![CDATA[ 





<section id="from-mechanical-to-digital" class="level2">
<h2 class="anchored" data-anchor-id="from-mechanical-to-digital">From Mechanical to Digital</h2>
<p>For over a century, the relationship between a driver and their vehicle was purely mechanical. When you turned the steering wheel, it pulled a cable to turn the wheels. When you pressed the pedal, it pushed a part inside the engine to make the car move. But in the last decade, the automobile has undergone a fundamental metamorphosis. It has evolved from a piece of heavy machinery into a sophisticated, mobile data center.</p>
<p>We are currently witnessing one of the most ambitious feats in the history of computer science: teaching a machine to “see” and navigate a chaotic, unpredictable world. This isn’t just about robotics; it is a masterclass in high-speed data processing and artificial intelligence. We’re not quite at the point where cars don’t need steering wheels yet, but we’re definitely in the age of “High Automation.” Today, vehicles can navigate complex downtown districts and highway merges with minimal human oversight.<span class="citation" data-cites="serban2020">(Serban, Poll, and Visser 2020)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://your-website-url.example.com/posts/Understanding the Road: Computer Vision Technology in Autonomous Vehicles/sae.png" class="img-fluid figure-img"></p>
<figcaption>SAE J3016 levels of driving automation.</figcaption>
</figure>
</div>
<p>But how does a car — a collection of glass, metal, and silicon—interpret a flickering shadow as a pedestrian or a yellow light as a command to slow down? The answer lies in the complex process behind computer vision.</p>
</section>
<section id="the-hardware-a-360-degree-data-stream" class="level2">
<h2 class="anchored" data-anchor-id="the-hardware-a-360-degree-data-stream">The Hardware: A 360-Degree Data Stream</h2>
<p>To understand the data science of autonomous vehicles, we must first look at the sensors. While some companies use Lidar (laser scanning) or Radar, the most “human-like” approach relies heavily on high-resolution cameras.</p>
<p>An autonomous vehicle is typically outfitted with a suite of eight or more cameras providing a seamless 360-degree view. These are not your standard dash-cams. They capture high-dynamic-range (HDR) data at high frame rates, generating gigabytes of raw visual information every minute. This constant stream of data is the “food” for the car’s artificial intelligence.</p>
<p>However, raw data is rarely “clean.” Just as a data scientist spends a significant portion of their time cleaning datasets, a car’s onboard computer must preprocess every frame. Images may be distorted by the curve of a wide-angle lens, blurred by high-speed movement, or obscured by the glare of a setting sun. Through automated image processing, the system adjusts brightness, sharpens edges, and corrects geometric distortions in real-time—ensuring that the downstream AI models receive the clearest possible signal.</p>
</section>
<section id="object-detection-the-power-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="object-detection-the-power-of-deep-learning">Object Detection: The Power of Deep Learning</h2>
<p>Once the image is “clean,” the vehicle faces its first major cognitive challenge: Object Detection. It must answer the question, “What am I looking at?”</p>
<p>This is where Deep Learning—specifically a architecture known as a Convolutional Neural Network (CNN)—comes into play. In the past, software engineers tried to write manual rules to identify objects (e.g., “If you see two circles and a frame, it’s a bicycle”). This failed miserably because the world is too diverse. A bicycle looks different from the side than from the front; it looks different in the rain or when partially hidden by a car.</p>
<p>Modern data science solves this through pattern recognition. These AI models have been “trained” on millions of labeled images. By processing vast datasets, the neural network learns to identify the fundamental features of a pedestrian, a stop sign, or a traffic cone. It recognizes the “features” of an object rather than a rigid definition. When a camera captures a person on a bicycle, the system identifies the pattern in milliseconds—faster than the human brain can consciously register the image.</p>
</section>
<section id="understanding-the-scene-semantic-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-scene-semantic-segmentation">Understanding the Scene: Semantic Segmentation</h2>
<p>Detecting an object is only half the battle. A car doesn’t just need to know what an object is; it needs to understand the context of the entire environment. This is achieved through a process called Semantic Segmentation.</p>
<p>In standard image recognition, a computer might put a box around a car. In semantic segmentation, the AI “color-codes” every single pixel in the image. It classifies every millimeter of the frame into categories:</p>
<ul>
<li>Drivable Surface: The asphalt directly in front of the car.</li>
<li>Sidewalk: A safe zone for pedestrians, but a “no-go” zone for the car.</li>
<li>Obstacles: Other vehicles, poles, or debris.</li>
<li>Dynamic Objects: Things that move, like dogs or children, which require object tracking. By segmenting the scene, the car creates a digital map of “free space.” It isn’t just seeing a picture; it is constructing a mathematical model of its surroundings <span class="citation" data-cites="vahagn2024">(Vahagn 2024)</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://your-website-url.example.com/posts/Understanding the Road: Computer Vision Technology in Autonomous Vehicles/car.jpg" class="img-fluid figure-img"></p>
<figcaption>How the car ‘see’ the world</figcaption>
</figure>
</div>
</section>
<section id="depth-perception-the-3d-world-in-a-2d-frame" class="level2">
<h2 class="anchored" data-anchor-id="depth-perception-the-3d-world-in-a-2d-frame">Depth Perception: The 3D World in a 2D Frame</h2>
<p>Humans have two eyes to perceive depth through “stereopsis.” Autonomous vehicles replicate this by using multiple cameras with overlapping fields of view. By comparing the slight difference in the position of an object between two cameras, the system can use geometry to calculate exactly how many meters away that object is.</p>
<p>But data science has pushed this even further. Engineers are now using “Monocular Depth Estimation,” where the AI is trained to estimate distance from a single camera lens based on its understanding of scale and perspective—much like how you can tell a car is far away even if you close one eye <span class="citation" data-cites="ming2021">(Ming et al. 2021)</span>.This redundancy is vital; if one camera is obscured, the car can still maintain a 3D understanding of the world.</p>
</section>
<section id="the-long-tail-problem-the-data-scientists-nightmare" class="level2">
<h2 class="anchored" data-anchor-id="the-long-tail-problem-the-data-scientists-nightmare">The “Long Tail” Problem: The Data Scientist’s Nightmare</h2>
<p>If the technology is so advanced, why don’t we have 100% autonomy everywhere yet? The answer lies in what data scientists call the “Long Tail.”</p>
<p>It is relatively easy to train a car to drive on a sunny day in suburban California. The “head” of the distribution—the common scenarios—is well-understood. The challenge is the “tail”: the rare, bizarre, and “out-of-distribution” events.</p>
<ul>
<li>Weather: Heavy snow can turn the world white, making it impossible for a camera to distinguish between the road and a snowbank.</li>
<li>Edge Cases: How does a car react to a person in a dinosaur costume crossing the street? Or a construction worker holding a “Stop” sign that isn’t a standard octagon?</li>
<li>Sensor Noise: A stray plastic bag blowing across the road might look like a solid obstacle to an over-cautious AI, causing “phantom braking.” In data science, model accuracy often follows the law of diminishing returns. Reaching 90% accuracy is straightforward; reaching 99.999%—the level required for human safety—requires an exponential increase in data and computing power<span class="citation" data-cites="koopman2018">(Koopman 2018)</span>.</li>
</ul>
</section>
<section id="the-human-element-ethics-and-liability" class="level2">
<h2 class="anchored" data-anchor-id="the-human-element-ethics-and-liability">The Human Element: Ethics and Liability</h2>
<p>Beyond the code and the cameras, autonomous vehicles face a hurdle that data science alone cannot solve: human society.</p>
<p>When a human driver makes a mistake, the legal framework is clear. But when an algorithm makes a choice, the waters become murky. If an autonomous vehicle is forced to choose between two unavoidable accidents, who decides the “priority”? This is the modern version of the “Trolley Problem,” and it is currently being debated by ethicists and legislators worldwide.</p>
<p>Current laws were written for a world where a person is always behind the wheel. As we transition to a world where the “driver” is a line of code, we must develop new frameworks for liability. Is the manufacturer responsible? The software developer? The passenger? Solving these questions is as critical to the future of AVs as improving the object detection algorithms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://your-website-url.example.com/posts/Understanding the Road: Computer Vision Technology in Autonomous Vehicles/trolley-problem.png" class="img-fluid figure-img"></p>
<figcaption>Trolley’s Problem</figcaption>
</figure>
</div>
</section>
<section id="the-road-ahead" class="level2">
<h2 class="anchored" data-anchor-id="the-road-ahead">The Road Ahead</h2>
<p>The journey toward full autonomy is a testament to the power of modern data science. We have moved from simple cruise control to machines that can “see,” “think,” and “act” in real-time. Every mile driven by these vehicles generates more data, which is fed back into the cloud to retrain and improve the models, creating a virtuous cycle of constant learning.</p>
<p>Perfection remains elusive, but the progress is undeniable. As cameras become more sensitive, algorithms more efficient, and our legal systems more adaptable, we are moving toward a future where the “driver’s seat” becomes just another place to sit and enjoy the ride. The autonomous revolution isn’t just coming; it’s being written in code, one pixel at a time.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-koopman2018" class="csl-entry">
Koopman, Philip. 2018. <span>“The Heavy Tail Safety Ceiling.”</span> 2018. <a href="https://users.ece.cmu.edu/~koopman/pubs/koopman18_heavy_tail_ceiling.pdf">https://users.ece.cmu.edu/~koopman/pubs/koopman18_heavy_tail_ceiling.pdf</a>.
</div>
<div id="ref-ming2021" class="csl-entry">
Ming, Yue et al. 2021. <span>“Deep Learning for Monocular Depth Estimation: A Review.”</span> <em>Neurocomputing</em> 438: 14–33.
</div>
<div id="ref-serban2020" class="csl-entry">
Serban, Alex, Erik Poll, and Joost Visser. 2020. <span>“A Standard Driven Software Architecture for Fully Autonomous Vehicles.”</span> <em>Journal of Automotive Software Engineering</em> 1. <a href="https://doi.org/10.2991/jase.d.200212.001">https://doi.org/10.2991/jase.d.200212.001</a>.
</div>
<div id="ref-vahagn2024" class="csl-entry">
Vahagn, Tumanyan. 2024. <span>“Complete Guide to Semantic Segmentation.”</span> 2024. <a href="https://www.superannotate.com/blog/guide-to-semantic-segmentation">https://www.superannotate.com/blog/guide-to-semantic-segmentation</a>.
</div>
</div></section></div> ]]></description>
  <category>explanation</category>
  <guid>https://your-website-url.example.com/posts/Understanding the Road: Computer Vision Technology in Autonomous Vehicles/</guid>
  <pubDate>Wed, 14 Jan 2026 08:00:00 GMT</pubDate>
  <media:content url="https://your-website-url.example.com/posts/Understanding the Road: Computer Vision Technology in Autonomous Vehicles/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Welcome to My Blog</title>
  <dc:creator>Raymond Wang</dc:creator>
  <link>https://your-website-url.example.com/posts/welcome/</link>
  <description><![CDATA[ 





<p>This is the first post in a Quarto blog. Welcome!</p>
<p><img src="https://your-website-url.example.com/posts/welcome/thumbnail.jpg" class="img-fluid"></p>
<p>Since this post doesn’t specify an explicit <code>image</code>, the first image in the post will be used in the listing page of posts.</p>



 ]]></description>
  <category>news</category>
  <guid>https://your-website-url.example.com/posts/welcome/</guid>
  <pubDate>Sun, 11 Jan 2026 08:00:00 GMT</pubDate>
</item>
</channel>
</rss>
