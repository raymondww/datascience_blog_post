<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raymond Wang">
<meta name="dcterms.date" content="2026-01-14">

<title>Understanding the Road: Computer Vision Technology in Autonomous Vehicles – Raymond Wang’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1c33c0780a01951c1c2d77b4a45239a1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Raymond Wang’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Understanding the Road: Computer Vision Technology in Autonomous Vehicles</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">explanation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Raymond Wang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 14, 2026</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="from-mechanical-to-digital" class="level2">
<h2 class="anchored" data-anchor-id="from-mechanical-to-digital">From Mechanical to Digital</h2>
<p>For over a century, the relationship between a driver and their vehicle was purely mechanical. When you turned the steering wheel, it pulled a cable to turn the wheels. When you pressed the pedal, it pushed a part inside the engine to make the car move. But in the last decade, the automobile has undergone a fundamental metamorphosis. It has evolved from a piece of heavy machinery into a sophisticated, mobile data center.</p>
<p>We are currently witnessing one of the most ambitious feats in the history of computer science: teaching a machine to “see” and navigate a chaotic, unpredictable world. This isn’t just about robotics; it is a masterclass in high-speed data processing and artificial intelligence. We’re not quite at the point where cars don’t need steering wheels yet, but we’re definitely in the age of “High Automation.” Today, vehicles can navigate complex downtown districts and highway merges with minimal human oversight.<span class="citation" data-cites="serban2020">(<a href="#ref-serban2020" role="doc-biblioref">Serban, Poll, and Visser 2020</a>)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sae.png" class="img-fluid figure-img"></p>
<figcaption>SAE J3016 levels of driving automation.</figcaption>
</figure>
</div>
<p>But how does a car — a collection of glass, metal, and silicon—interpret a flickering shadow as a pedestrian or a yellow light as a command to slow down? The answer lies in the complex process behind computer vision.</p>
</section>
<section id="the-hardware-a-360-degree-data-stream" class="level2">
<h2 class="anchored" data-anchor-id="the-hardware-a-360-degree-data-stream">The Hardware: A 360-Degree Data Stream</h2>
<p>To understand the data science of autonomous vehicles, we must first look at the sensors. While some companies use Lidar (laser scanning) or Radar, the most “human-like” approach relies heavily on high-resolution cameras.</p>
<p>An autonomous vehicle is typically outfitted with a suite of eight or more cameras providing a seamless 360-degree view. These are not your standard dash-cams. They capture high-dynamic-range (HDR) data at high frame rates, generating gigabytes of raw visual information every minute. This constant stream of data is the “food” for the car’s artificial intelligence.</p>
<p>However, raw data is rarely “clean.” Just as a data scientist spends a significant portion of their time cleaning datasets, a car’s onboard computer must preprocess every frame. Images may be distorted by the curve of a wide-angle lens, blurred by high-speed movement, or obscured by the glare of a setting sun. Through automated image processing, the system adjusts brightness, sharpens edges, and corrects geometric distortions in real-time—ensuring that the downstream AI models receive the clearest possible signal.</p>
</section>
<section id="object-detection-the-power-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="object-detection-the-power-of-deep-learning">Object Detection: The Power of Deep Learning</h2>
<p>Once the image is “clean,” the vehicle faces its first major cognitive challenge: Object Detection. It must answer the question, “What am I looking at?”</p>
<p>This is where Deep Learning—specifically a architecture known as a Convolutional Neural Network (CNN)—comes into play. In the past, software engineers tried to write manual rules to identify objects (e.g., “If you see two circles and a frame, it’s a bicycle”). This failed miserably because the world is too diverse. A bicycle looks different from the side than from the front; it looks different in the rain or when partially hidden by a car.</p>
<p>Modern data science solves this through pattern recognition. These AI models have been “trained” on millions of labeled images. By processing vast datasets, the neural network learns to identify the fundamental features of a pedestrian, a stop sign, or a traffic cone. It recognizes the “features” of an object rather than a rigid definition. When a camera captures a person on a bicycle, the system identifies the pattern in milliseconds—faster than the human brain can consciously register the image.</p>
</section>
<section id="understanding-the-scene-semantic-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-scene-semantic-segmentation">Understanding the Scene: Semantic Segmentation</h2>
<p>Detecting an object is only half the battle. A car doesn’t just need to know what an object is; it needs to understand the context of the entire environment. This is achieved through a process called Semantic Segmentation.</p>
<p>In standard image recognition, a computer might put a box around a car. In semantic segmentation, the AI “color-codes” every single pixel in the image. It classifies every millimeter of the frame into categories:</p>
<ul>
<li>Drivable Surface: The asphalt directly in front of the car.</li>
<li>Sidewalk: A safe zone for pedestrians, but a “no-go” zone for the car.</li>
<li>Obstacles: Other vehicles, poles, or debris.</li>
<li>Dynamic Objects: Things that move, like dogs or children, which require object tracking. By segmenting the scene, the car creates a digital map of “free space.” It isn’t just seeing a picture; it is constructing a mathematical model of its surroundings <span class="citation" data-cites="vahagn2024">(<a href="#ref-vahagn2024" role="doc-biblioref">Vahagn 2024</a>)</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="car.jpg" class="img-fluid figure-img"></p>
<figcaption>How the car ‘see’ the world</figcaption>
</figure>
</div>
</section>
<section id="depth-perception-the-3d-world-in-a-2d-frame" class="level2">
<h2 class="anchored" data-anchor-id="depth-perception-the-3d-world-in-a-2d-frame">Depth Perception: The 3D World in a 2D Frame</h2>
<p>Humans have two eyes to perceive depth through “stereopsis.” Autonomous vehicles replicate this by using multiple cameras with overlapping fields of view. By comparing the slight difference in the position of an object between two cameras, the system can use geometry to calculate exactly how many meters away that object is.</p>
<p>But data science has pushed this even further. Engineers are now using “Monocular Depth Estimation,” where the AI is trained to estimate distance from a single camera lens based on its understanding of scale and perspective—much like how you can tell a car is far away even if you close one eye <span class="citation" data-cites="ming2021">(<a href="#ref-ming2021" role="doc-biblioref">Ming et al. 2021</a>)</span>.This redundancy is vital; if one camera is obscured, the car can still maintain a 3D understanding of the world.</p>
</section>
<section id="the-long-tail-problem-the-data-scientists-nightmare" class="level2">
<h2 class="anchored" data-anchor-id="the-long-tail-problem-the-data-scientists-nightmare">The “Long Tail” Problem: The Data Scientist’s Nightmare</h2>
<p>If the technology is so advanced, why don’t we have 100% autonomy everywhere yet? The answer lies in what data scientists call the “Long Tail.”</p>
<p>It is relatively easy to train a car to drive on a sunny day in suburban California. The “head” of the distribution—the common scenarios—is well-understood. The challenge is the “tail”: the rare, bizarre, and “out-of-distribution” events.</p>
<ul>
<li>Weather: Heavy snow can turn the world white, making it impossible for a camera to distinguish between the road and a snowbank.</li>
<li>Edge Cases: How does a car react to a person in a dinosaur costume crossing the street? Or a construction worker holding a “Stop” sign that isn’t a standard octagon?</li>
<li>Sensor Noise: A stray plastic bag blowing across the road might look like a solid obstacle to an over-cautious AI, causing “phantom braking.” In data science, model accuracy often follows the law of diminishing returns. Reaching 90% accuracy is straightforward; reaching 99.999%—the level required for human safety—requires an exponential increase in data and computing power<span class="citation" data-cites="koopman2018">(<a href="#ref-koopman2018" role="doc-biblioref">Koopman 2018</a>)</span>.</li>
</ul>
</section>
<section id="the-human-element-ethics-and-liability" class="level2">
<h2 class="anchored" data-anchor-id="the-human-element-ethics-and-liability">The Human Element: Ethics and Liability</h2>
<p>Beyond the code and the cameras, autonomous vehicles face a hurdle that data science alone cannot solve: human society.</p>
<p>When a human driver makes a mistake, the legal framework is clear. But when an algorithm makes a choice, the waters become murky. If an autonomous vehicle is forced to choose between two unavoidable accidents, who decides the “priority”? This is the modern version of the “Trolley Problem,” and it is currently being debated by ethicists and legislators worldwide.</p>
<p>Current laws were written for a world where a person is always behind the wheel. As we transition to a world where the “driver” is a line of code, we must develop new frameworks for liability. Is the manufacturer responsible? The software developer? The passenger? Solving these questions is as critical to the future of AVs as improving the object detection algorithms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="trolley-problem.png" class="img-fluid figure-img"></p>
<figcaption>Trolley’s Problem</figcaption>
</figure>
</div>
</section>
<section id="the-road-ahead" class="level2">
<h2 class="anchored" data-anchor-id="the-road-ahead">The Road Ahead</h2>
<p>The journey toward full autonomy is a testament to the power of modern data science. We have moved from simple cruise control to machines that can “see,” “think,” and “act” in real-time. Every mile driven by these vehicles generates more data, which is fed back into the cloud to retrain and improve the models, creating a virtuous cycle of constant learning.</p>
<p>Perfection remains elusive, but the progress is undeniable. As cameras become more sensitive, algorithms more efficient, and our legal systems more adaptable, we are moving toward a future where the “driver’s seat” becomes just another place to sit and enjoy the ride. The autonomous revolution isn’t just coming; it’s being written in code, one pixel at a time.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-koopman2018" class="csl-entry" role="listitem">
Koopman, Philip. 2018. <span>“The Heavy Tail Safety Ceiling.”</span> 2018. <a href="https://users.ece.cmu.edu/~koopman/pubs/koopman18_heavy_tail_ceiling.pdf">https://users.ece.cmu.edu/~koopman/pubs/koopman18_heavy_tail_ceiling.pdf</a>.
</div>
<div id="ref-ming2021" class="csl-entry" role="listitem">
Ming, Yue et al. 2021. <span>“Deep Learning for Monocular Depth Estimation: A Review.”</span> <em>Neurocomputing</em> 438: 14–33.
</div>
<div id="ref-serban2020" class="csl-entry" role="listitem">
Serban, Alex, Erik Poll, and Joost Visser. 2020. <span>“A Standard Driven Software Architecture for Fully Autonomous Vehicles.”</span> <em>Journal of Automotive Software Engineering</em> 1. <a href="https://doi.org/10.2991/jase.d.200212.001">https://doi.org/10.2991/jase.d.200212.001</a>.
</div>
<div id="ref-vahagn2024" class="csl-entry" role="listitem">
Vahagn, Tumanyan. 2024. <span>“Complete Guide to Semantic Segmentation.”</span> 2024. <a href="https://www.superannotate.com/blog/guide-to-semantic-segmentation">https://www.superannotate.com/blog/guide-to-semantic-segmentation</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/your-website-url\.example\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>